<!doctype html><html lang="en"><head><title>Minimizing with Gradient Descent - Jim Killingsworth</title><meta charset="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="The previous post demonstrates the use of a hill climbing algorithm to find a set of parameters that minimize a cost function associated with a coin toss game. In this post, I want to explore the use of a gradient descent algorithm as an alternative. The two classes of algorithms are very similar in that they both iteratively update an estimated parameter set. But while the hill climbing algorithm only updates one parameter at a time, the gradient descent approach updates all parameters in proportion to the direction of steepest descent."/><base href="/"/><link rel="canonical" href="https://jkillingsworth.com/2020/09/12/minimizing-with-gradient-descent/"/><link rel="icon" href="./static/favicon.ico" type="image/x-icon"/><link rel="icon" href="./static/favicon-256.jpg" sizes="256x256"/><link rel="preload" href="./static/fonts/open-sans-v20-latin-700.woff2" as="font" crossorigin/><link rel="preload" href="./static/fonts/lora-v17-latin-regular.woff2" as="font" crossorigin/><link rel="preload" href="./static/fonts/lora-v17-latin-italic.woff2" as="font" crossorigin/><link rel="preload" href="./static/fonts/roboto-mono-v13-latin-regular.woff2" as="font" crossorigin/><link rel="preload" href="https://www.google-analytics.com/analytics.js" as="script"/><script src="https://www.googletagmanager.com/gtag/js?id=UA-114299226-1" async></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag("js", new Date());
  gtag("config", "UA-114299226-1", { "send_page_view": true });
</script><style>@font-face{font-display:swap;font-family:"Open Sans";font-style:normal;font-weight:700;src:local("Open Sans Bold"),local("OpenSans-Bold"),url(./static/fonts/open-sans-v20-latin-700.woff2) format("woff2"),url(./static/fonts/open-sans-v20-latin-700.woff) format("woff")}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:400;src:local("Lora Regular"),local("Lora-Regular"),url(./static/fonts/lora-v17-latin-regular.woff2) format("woff2"),url(./static/fonts/lora-v17-latin-regular.woff) format("woff")}@font-face{font-display:swap;font-family:"Lora";font-style:italic;font-weight:400;src:local("Lora Italic"),local("Lora-Italic"),url(./static/fonts/lora-v17-latin-italic.woff2) format("woff2"),url(./static/fonts/lora-v17-latin-italic.woff) format("woff")}@font-face{font-display:swap;font-family:"Roboto Mono";font-style:normal;font-weight:400;src:local("Roboto Mono"),local("RobotoMono-Regular"),url(./static/fonts/roboto-mono-v13-latin-regular.woff2) format("woff2"),url(./static/fonts/roboto-mono-v13-latin-regular.woff) format("woff")}
/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:0.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace, monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace, monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,[type="button"],[type="reset"],[type="submit"]{-webkit-appearance:button}button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type="button"]:-moz-focusring,[type="reset"]:-moz-focusring,[type="submit"]:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:0.35em 0.75em 0.625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0}[type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{height:auto}[type="search"]{-webkit-appearance:textfield;outline-offset:-2px}[type="search"]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}template{display:none}[hidden]{display:none}
html{box-sizing:border-box;overflow-y:scroll}*,*::before,*::after{box-sizing:inherit}body{font-family:"Lora", "Georgia", serif;font-weight:normal;background:#fff;color:#000;display:flex;flex-flow:row wrap;font-size:12pt;justify-content:space-between;line-height:1.5;margin:0 auto;width:90%}body>*{flex:0 100%}body>header{font-size:21pt;margin:10px 0}body>footer{border-top:3px solid gray;font-size:10pt;font-style:italic;padding:16px 0}header,nav,h1,h2{font-family:"Open Sans", "Verdana", sans-serif;font-weight:bold}header a,nav a,h1 a,h2 a{color:inherit}a{color:blue;text-decoration:none}a:hover{color:blue;text-decoration:underline}strong{font-family:"Open Sans", "Verdana", sans-serif;font-weight:bold}svg{fill:currentColor}.navmenu{border-top:3px solid gray;border-bottom:1px solid #ccc}.navmenu #toggle{float:right;height:24px;margin:4px 0;width:24px}.navmenu #toggle+label{color:transparent;cursor:pointer;display:block;padding:4px 0;position:relative;user-select:none;-moz-user-select:none;-ms-user-select:none;-webkit-user-select:none}.navmenu #toggle+label::before{background:#fff;background-size:24px;content:"";float:right;height:24px;left:24px;position:relative;width:24px}.navmenu #toggle+label::after{content:"Menu";left:0;padding:0 16px;position:absolute}.navmenu #toggle+label{border-bottom:none}.navmenu #toggle+label::before{background-image:url("./static/chevron-down.svg")}.navmenu #toggle+label::after{color:#000}.navmenu #toggle~ul{height:0;visibility:hidden}.navmenu #toggle:checked+label{border-bottom:1px solid #e6e6e6}.navmenu #toggle:checked+label::before{background-image:url("./static/chevron-up.svg")}.navmenu #toggle:checked+label::after{color:gray}.navmenu #toggle:checked~ul{height:auto;visibility:visible}.navmenu ul{display:flex;flex-flow:column wrap;list-style-type:none;justify-content:flex-start;margin:0;overflow:hidden;padding:0}.navmenu ul>li{flex:none}.navmenu ul>li a{display:block;padding:4px 16px}.navmenu ul>li a:hover,.navmenu ul>li a:focus{background:gray;color:#fff}.content{max-width:720px}.content header,.content footer,.content h1,.content h2,.content p,.content ul,.content figure{margin:24px 0}.content h1{font-size:18pt}.content h2{font-size:12pt}.content footer{font-style:italic}.content p:last-of-type::after{clear:both;content:"";display:block}.content ul{padding-left:16px}.content ul>li{margin:12px 0}.content figure{max-width:90vw}.content figure>img{display:block}.content .fig-chart{padding-top:56.25%;page-break-inside:avoid;position:relative}.content .fig-chart>img{max-height:100%;max-width:100%;position:absolute;top:0}.content .fig-latex{overflow-x:auto}.content .fig-latex>img{margin:0 16px}.content .fig-pic-l>img,.content .fig-pic-r>img{margin:0 auto;max-width:100%;object-fit:cover}.content .button{font-family:"Open Sans", "Verdana", sans-serif;font-weight:bold;background:#e6e6e6;color:inherit;display:block;margin:24px 0;padding:4px 0;text-align:center}.content .button:hover,.content .button:focus{background:gray;color:#fff}.content .message header>h1{background:#ff0;display:inline-block;font-size:12pt;margin:0;text-transform:lowercase}.content .message header>h1::first-letter{text-transform:uppercase}.content .message header>h1::after{content:":"}.content .message p{font-style:italic}.content .newpost header>h1{background:#ff0;display:inline-block;font-size:12pt;margin:0;text-transform:lowercase}.content .newpost header>h1::first-letter{text-transform:uppercase}.content .newpost header>h1::after{content:":"}.content .newpost header>h2{font-size:18pt}.content .newpost a.readmore::after{content:" \00bb"}.content .newpost a:not(.readmore):not(:hover){color:inherit}.content .oldpost{margin:24px 0}.content .oldpost header>h1{display:none}.content .archive ul>li time{font-family:"Roboto Mono", "Consolas", "Menlo", monospace;font-weight:normal;display:none;font-size:10pt}.content .contact label,.content .contact input[type="submit"]{font-family:"Open Sans", "Verdana", sans-serif;font-weight:bold;cursor:pointer;display:block;margin:16px 0}.content .contact label{margin-bottom:4px}.content .contact input[type="text"],.content .contact input[type="email"],.content .contact textarea{font-family:"Roboto Mono", "Consolas", "Menlo", monospace;font-weight:normal;display:block;font-size:10pt;line-height:inherit;padding:4px;width:100%}.content .contact textarea{height:200px;max-width:100%;min-width:100%}.content .contact input[type="submit"]{padding:4px 40px}.content .comments{margin:24px 0 8px}.content .comments header>h2{display:none}.content .comments noscript{font-family:"Roboto Mono", "Consolas", "Menlo", monospace;font-weight:normal;display:block;font-size:10pt;margin:-12px 0 24px}.sidebar header>h1{display:none}.sidebar header>h2{border-bottom:1px solid #ccc;border-top:3px solid gray;font-size:inherit;margin:0;padding:4px 0}.sidebar section{margin:24px 0}.sidebar section:first-of-type{margin-top:0}.sidebar .social-media ul{list-style:none;padding:0 0 0 16px}.sidebar .social-media ul>li a{display:inline-block}.sidebar .social-media ul>li a *{vertical-align:middle}.sidebar .social-media ul>li a span{margin:0 10px}.sidebar .social-media ul>li a svg{color:gray;height:48px;width:48px}.sidebar .social-media ul>li a:hover svg{color:inherit}.content p:not(.nojustify),.sidebar p:not(.nojustify){text-align:justify;word-spacing:-1px}.content p,.content ul,.sidebar p,.sidebar ul{line-height:1.667}@media only screen and (min-width: 600px){.content ul{padding-left:40px}.content ul>li{margin:0}.content figure{max-width:90vw}.content .fig-latex>img{margin:0 40px}.content .fig-pic-l>img{float:left;margin:0 24px 0 0}.content .fig-pic-r>img{float:right;margin:0 0 0 24px}.content .archive ul>li time{display:inline}body{max-width:720px;width:90%}body>header{font-size:24pt}.navmenu #toggle{display:none}.navmenu #toggle+label{display:none}.navmenu #toggle~ul{height:auto;visibility:visible}.navmenu ul{flex-flow:row wrap}.navmenu ul>li a{padding:4px 32px}}@media only screen and (min-width: 1200px){body{max-width:1200px}body>header{text-align:center}.content{flex:0 60%}.sidebar{flex:0 35%}.sidebar section:first-of-type{margin-top:40px}}@media only print{.content ul{padding-left:40px}.content ul>li{margin:0}.content figure{max-width:100vw}.content .fig-latex>img{margin:0 40px}.content .fig-pic-l>img{float:left;margin:0 24px 0 0}.content .fig-pic-r>img{float:right;margin:0 0 0 24px}.content .archive ul>li time{display:inline}body{display:block;margin:auto;width:100%}body>header{border-bottom:1px solid #ccc;border-top:3px solid gray;font-size:inherit;margin:0;padding:4px 0;text-align:left}.navmenu,.sidebar,.oldpost,.comments{display:none}}@page{margin:1in 1.25in}
</style></head><body><header><a href="./">Jim Killingsworth</a></header><nav class="navmenu"><input id="toggle" type="checkbox"/><label for="toggle">Enable menu</label><ul><li><a href="./"> Home </a></li><li><a href="./archive/"> Archive </a></li><li><a href="./about/"> About </a></li><li><a href="./contact/"> Contact </a></li></ul></nav><main class="content"><article><header><h1>Minimizing with Gradient Descent</h1></header><p>The <a href="/2020/08/16/visualizing-the-climb-up-the-hill/">pre­vi­ous post</a> demon­strates the use of a hill climb­ing algo­rithm to find a set of para­me­ters that min­i­mize a cost func­tion asso­ci­ated with a coin toss game. In this post, I want to explore the use of a gra­di­ent descent algo­rithm as an alter­na­tive. The two classes of algo­rithms are very sim­i­lar in that they both iter­a­tively update an esti­mated para­me­ter set. But while the hill climb­ing algo­rithm only updates one para­me­ter at a time, the gra­di­ent descent approach updates all para­me­ters in pro­por­tion to the direc­tion of steep­est descent.</p><h2 id="the-algorithm">The Algorithm</h2><p>The gra­di­ent descent algo­rithm is a way of find­ing a local min­i­mum of a dif­fer­en­tiable func­tion. In this case, we’ll use the same cost func­tion used in the pre­vi­ous post titled <a href="/2020/08/16/visualizing-the-climb-up-the-hill/"><em>Visu­al­iz­ing the Climb up the Hill</em></a> as the exam­ple. Recall the fol­low­ing equa­tions based on the model of the coin toss game:</p><figure class="fig-latex"><img width="144" height="83" alt="Figure 1" src="./2020/09/12/minimizing-with-gradient-descent/fig-01-latex-2958754018.svg"></figure><p>The two equa­tions above must hold true if we have a valid set of weights for a given tar­get dis­tri­b­u­tion. If we esti­mate a set of weights that do not sat­isfy these equa­tions, then we know the esti­mate can be improved. The sum of squared errors makes for a con­ve­nient cost function:</p><figure class="fig-latex"><img width="381" height="33" alt="Figure 2" src="./2020/09/12/minimizing-with-gradient-descent/fig-02-latex-1667404440.svg"></figure><p>Note the use of a para­me­ter vec­tor for the input. As you might expect, this is a con­ve­nient nota­tion for mul­ti­vari­able func­tions. Now once we have a cost func­tion to work with, we also need to find the gra­di­ent of the cost func­tion:</p><figure class="fig-latex"><img width="145" height="108" alt="Figure 3" src="./2020/09/12/minimizing-with-gradient-descent/fig-03-latex-2729400786.svg"></figure><p>The gra­di­ent is a vec­tor con­tain­ing par­tial deriv­a­tives of the cost func­tion, one for each of the vari­ables. Each com­po­nent is the slope of the func­tion along the cor­re­spond­ing axis. The vec­tor points in the direc­tion of steep­est ascent. The apply the gra­di­ent descent algo­rithm, we need to sub­tract a value in the direc­tion of the gra­di­ent from an esti­mated value to obtain an improved val­ue. Here is the formula:</p><figure class="fig-latex"><img width="166" height="19" alt="Figure 4" src="./2020/09/12/minimizing-with-gradient-descent/fig-04-latex-3657126264.svg"></figure><p>Start­ing with an ini­tial guess, we can apply the above repeat­edly until the gra­di­ent has a mag­ni­tude of zero. Once the gra­di­ent is zero, we know we have reached a local min­i­mum. In prac­tice, how­ev­er, you might want to stop once the gra­di­ent is suf­fi­ciently small. In this exam­ple, we ter­mi­nate once the mag­ni­tude of the gra­di­ent is below a pre­de­ter­mined step size:</p><figure class="fig-latex"><img width="81" height="14" alt="Figure 5" src="./2020/09/12/minimizing-with-gradient-descent/fig-05-latex-0280775991.svg"></figure><p>To ensure each incre­ment adjusts the esti­mate by a fixed step size, we need to mul­ti­ply the gra­di­ent by a scalar fac­tor called the learn­ing rate:</p><figure class="fig-latex"><img width="117" height="39" alt="Figure 6" src="./2020/09/12/minimizing-with-gradient-descent/fig-06-latex-2817687068.svg"></figure><p>In this instance, the learn­ing rate is sim­ply the step size divided by the mag­ni­tude of the gra­di­ent vec­tor. The learn­ing rate is recom­puted each iteration.</p><h2 id="visualizations">Visualizations</h2><p>I want to com­pare and con­trast the gra­di­ent descent algo­rithm out­lined above with the hill climb­ing algo­rithms used in the pre­vi­ous post. To do so, let’s use the same tar­get dis­tri­b­u­tion as we used before:</p><figure class="fig-chart" style="padding-top: 56.25%;"><img width="720" height="405" alt="Figure 7" src="./2020/09/12/minimizing-with-gradient-descent/fig-07-target-pmfunc.svg"></figure><p>These val­ues can be rep­re­sented sym­bol­i­cally as like this:</p><figure class="fig-latex"><img width="89" height="44" alt="Figure 8" src="./2020/09/12/minimizing-with-gradient-descent/fig-08-latex-1092596754.svg"></figure><p>These val­ues can be used to pro­duce a con­crete form of the cost function:</p><figure class="fig-latex"><img width="455" height="33" alt="Figure 9" src="./2020/09/12/minimizing-with-gradient-descent/fig-09-latex-3780367009.svg"></figure><p>Just like with the hill climb­ing algo­rithms used in the pre­vi­ous post, to apply the gra­di­ent descent algo­rithm we need to start out with an ini­tial esti­mate. Let’s use the fol­low­ing set of para­me­ters as the ini­tial guess:</p><figure class="fig-latex"><img width="153" height="71" alt="Figure 10" src="./2020/09/12/minimizing-with-gradient-descent/fig-10-latex-2626067728.svg"></figure><p>Apply­ing the gra­di­ent descent pro­ce­dure using the above as the ini­tial input, the out­put of the first iter­a­tion is the input to the sec­ond iter­a­tion. The esti­mate is updated repeat­edly until the process ter­mi­nates. Here is the final result:</p><figure class="fig-latex"><img width="153" height="71" alt="Figure 11" src="./2020/09/12/minimizing-with-gradient-descent/fig-11-latex-2139325690.svg"></figure><p>The algo­rithm ter­mi­nates after 4,333 iter­a­tions. The fol­low­ing trace shows the path it takes from start to finish:</p><figure class="fig-chart" style="padding-top: 56.25%;"><img width="720" height="405" alt="Figure 12" src="./2020/09/12/minimizing-with-gradient-descent/fig-12-trace-1-heatmap.svg"></figure><p>Using the same tech­nique, we can run through sev­eral more exam­ples, each with dif­fer­ent start­ing points:</p><figure class="fig-chart" style="padding-top: 56.25%;"><img width="720" height="405" alt="Figure 13" src="./2020/09/12/minimizing-with-gradient-descent/fig-13-trace-2-heatmap.svg"></figure><figure class="fig-chart" style="padding-top: 56.25%;"><img width="720" height="405" alt="Figure 14" src="./2020/09/12/minimizing-with-gradient-descent/fig-14-trace-3-heatmap.svg"></figure><figure class="fig-chart" style="padding-top: 56.25%;"><img width="720" height="405" alt="Figure 15" src="./2020/09/12/minimizing-with-gradient-descent/fig-15-trace-4-heatmap.svg"></figure><p>As you can see, the final result very much depends on the ini­tial esti­mate. In each case, the path is a smooth and slightly curved line from start to fin­ish. Notice how in each case, the tra­jec­tory is sim­i­lar to that of the sto­chas­tic hill climb­ing algo­rithm used in the pre­vi­ous post. When using the same step size, the gra­di­ent descent approach requires around 70% to 80% of the num­ber of iter­a­tions to com­plete when com­pared to the hill climb­ing alternative.</p><h2 id="learning-rate">Learning Rate</h2><p>Each one of the exam­ples illus­trated above requires over a thou­sand iter­a­tions to com­plete. Some of them require over four thou­sand iter­a­tions. The num­ber of iter­a­tions required depends largely on the learn­ing rate. I think it’s worth point­ing out here that other learn­ing rate schemes are pos­si­ble. Some learn­ing rates may yield a much faster con­ver­gence than the one used here. Con­sider this alternative:</p><figure class="fig-latex"><img width="48" height="17" alt="Figure 16" src="./2020/09/12/minimizing-with-gradient-descent/fig-16-latex-1706916876.svg"></figure><p>With the cost func­tion used in the exam­ples above, this alter­na­tive learn­ing rate speeds up the rate of con­ver­gence by at least two orders of mag­ni­tude. In the small toy exam­ples illus­trated here, this isn’t going to make any tan­gi­ble dif­fer­ence; it already exe­cutes fast enough. But in a larger appli­ca­tion, an opti­mized learn­ing rate can have a sig­nif­i­cant impact on the run­time per­for­mance of the gra­di­ent descent algorithm.</p><p class="nojustify"><a href="https://github.com/jkillingsworth/jkillingsworth.com/tree/master/src/2020-09-12-minimizing-with-gradient-descent">Accom­pa­ny­ing source code is avail­able on GitHub.</a></p><footer><time datetime="2020-09-12">September 12, 2020</time></footer></article><section class="comments"><header><h2>Comments</h2></header><script>
    var disqus_config = function () {
      this.page.url = "https://jkillingsworth.com/2020/09/12/minimizing-with-gradient-descent/";
      this.page.title = "Minimizing with Gradient Descent";
      this.page.identifier = "/2020/09/12/minimizing-with-gradient-descent/";
    };
    function disqus_show() {
      var d = document, s = d.createElement("script");
      s.src = "https://jkillingsworth.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    }
    function anchor_blur() {
      var a = document.getElementById("comments");
      a.innerText = "Comments";
      a.blur();
    }
    function disqus() {
      disqus_show();
      anchor_blur();
    }
  </script><a href="javascript:disqus();" id="comments" class="button">Show comments</a><div id="disqus_thread"></div><noscript> Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></section></main><aside class="sidebar"><header><h1>Sidebar</h1></header><section class="social-media"><header><h2>Social Media</h2></header><ul><li><a href="https://www.linkedin.com/in/jkillingsworth/"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 512"><path d="M417.2 64H96.8C79.3 64 64 76.6 64 93.9V415c0 17.4 15.3 32.9 32.8 32.9h320.3c17.6 0 30.8-15.6 30.8-32.9V93.9C448 76.6 434.7 64 417.2 64zM183 384h-55V213h55v171zm-25.6-197h-.4c-17.6 0-29-13.1-29-29.5 0-16.7 11.7-29.5 29.7-29.5s29 12.7 29.4 29.5c0 16.4-11.4 29.5-29.7 29.5zM384 384h-55v-93.5c0-22.4-8-37.7-27.9-37.7-15.2 0-24.2 10.3-28.2 20.3-1.5 3.6-1.9 8.5-1.9 13.5V384h-55V213h55v23.8c8-11.4 20.5-27.8 49.6-27.8 36.1 0 63.4 23.8 63.4 75.1V384z"></path></svg><span>LinkedIn</span></a></li><li><a href="https://github.com/jkillingsworth/"><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 512 512"><path d="M256 32C132.3 32 32 134.9 32 261.7c0 101.5 64.2 187.5 153.2 217.9 1.4.3 2.6.4 3.8.4 8.3 0 11.5-6.1 11.5-11.4 0-5.5-.2-19.9-.3-39.1-8.4 1.9-15.9 2.7-22.6 2.7-43.1 0-52.9-33.5-52.9-33.5-10.2-26.5-24.9-33.6-24.9-33.6-19.5-13.7-.1-14.1 1.4-14.1h.1c22.5 2 34.3 23.8 34.3 23.8 11.2 19.6 26.2 25.1 39.6 25.1 10.5 0 20-3.4 25.6-6 2-14.8 7.8-24.9 14.2-30.7-49.7-5.8-102-25.5-102-113.5 0-25.1 8.7-45.6 23-61.6-2.3-5.8-10-29.2 2.2-60.8 0 0 1.6-.5 5-.5 8.1 0 26.4 3.1 56.6 24.1 17.9-5.1 37-7.6 56.1-7.7 19 .1 38.2 2.6 56.1 7.7 30.2-21 48.5-24.1 56.6-24.1 3.4 0 5 .5 5 .5 12.2 31.6 4.5 55 2.2 60.8 14.3 16.1 23 36.6 23 61.6 0 88.2-52.4 107.6-102.3 113.3 8 7.1 15.2 21.1 15.2 42.5 0 30.7-.3 55.5-.3 63 0 5.4 3.1 11.5 11.4 11.5 1.2 0 2.6-.1 4-.4C415.9 449.2 480 363.1 480 261.7 480 134.9 379.7 32 256 32z"></path></svg><span>GitHub</span></a></li></ul></section></aside><footer> &copy; 2018&ndash;2021 Jim Killingsworth. All rights reserved. </footer></body></html>